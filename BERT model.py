# -*- coding: utf-8 -*-
"""
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yRqmoMJPLpfUk-q5dt2Tt-kQ-8_0UJE2
"""

# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VeQHYcL_Srdo34hmHjnTPfnjlN6Ig6k7
"""
"""code was adopted from https://github.com/jdfoote/Intro-to-Programming-and-Data-Science/blob/fall2021/extra_topics/twitter_v2_example.ipynb"""

#pip install tweepy

import tweepy

bearer_token="twitter API bearer token"

import time

import pandas as pd

client=tweepy.Client(bearer_token,wait_on_rate_limit=True)

AV_tweets = []
for response in tweepy.Paginator(client.search_all_tweets, 
                                 query = '"Self-driving" OR "driverless car" OR "Autonomous Vehicle" -is:retweet lang:en',
                                 user_fields = ['username', 'public_metrics', 'description', 'location'],
                                 tweet_fields = ['created_at', 'geo', 'public_metrics', 'text'],
                                 expansions = 'author_id',
                                 start_time = '2019-01-01T00:00:00Z',
                                 end_time = '2021-06-30T00:00:00Z',
                              max_results=500):
    time.sleep(1)
    AV_tweets.append(response)

AV_tweets[0].data[1]

AV_tweets[0].includes['users'][2]

AV_tweets[0].includes['users'][2].description

result = []
user_dict = {}
# Loop through each response object
for response in AV_tweets:
    # Take all of the users, and put them into a dictionary of dictionaries with the info we want to keep
    for user in response.includes['users']:
        user_dict[user.id] = {'username': user.username, 
                              'followers': user.public_metrics['followers_count'],
                              'tweets': user.public_metrics['tweet_count'],
                              'description': user.description,
                              'location': user.location
                             }
    for tweet in response.data:
        # For each tweet, find the author's information
        author_info = user_dict[tweet.author_id]
        # Put all of the information we want to keep in a single dictionary for each tweet
        result.append({'author_id': tweet.author_id, 
                       'username': author_info['username'],
                       'author_followers': author_info['followers'],
                       'author_tweets': author_info['tweets'],
                       'author_description': author_info['description'],
                       'author_location': author_info['location'],
                       'text': tweet.text,
                       'created_at': tweet.created_at,
                       'retweets': tweet.public_metrics['retweet_count'],
                       'replies': tweet.public_metrics['reply_count'],
                       'likes': tweet.public_metrics['like_count'],
                       'quote_count': tweet.public_metrics['quote_count']
                      })

df = pd.DataFrame(result)

df.text[1009]

df = df.drop(['author_followers','author_tweets','author_description','author_location', 'replies','quote_count'],axis = 1)

df.to_csv('tweets.csv',encoding="utf-8")

# -*- coding: utf-8 -*-
"""Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10T_rGAWKHbJwmNYQjj-5XkeCzx2aGV7l

# Tweet Cleaning
"""

import pandas as pd

#import_colleted_twitter_data
df = pd.read_csv('/content/drive/MyDrive/CS560/tweets.csv')

df.columns

#drop_unwanted_columns
df = df.drop(['Unnamed: 0'],axis = 1)
df

def clean_text(orig_text):
 import re, string
 # Remove URLs
 clean_text = re.sub(r"http\S+", "", orig_text)
 # Remove at-signs
 clean_text = re.sub(r"(?<=^|(?<=[^a-zA-Z0-9-_\.]))@([A-Za0-9-_]+)", "", clean_text)
 # Remove punctuation
 clean_text = clean_text.translate(str.maketrans(dict.fromkeys(string.punctuation)))
 # Replace linebreaks with spaces
 clean_text = re.sub(r"(\r?\n|\r)", " ", clean_text)
 # Strip non-ASCII characters
 clean_text = re.sub(r"[^\x00-\x7F]+","", clean_text)
 return clean_text;

df['text']=df['text'].apply(str)
df['cleaned'] = df['text'].apply(lambda x: clean_text(x))

df

#create_copy_of _the_data
df1 = df.copy()
df1.head()

#Appliyng tokenization
def tokenization(text):
    text = re.split('\W+', text)
    return text
df1['tokenized'] = df1['cleaned'].apply(lambda x: tokenization(x.lower()))

df1

import nltk
nltk.download('stopwords')

#Defining and removing stopwords
import nltk
stopword = nltk.corpus.stopwords.words('english')
def remove_stopwords(text):
    text= [word for word in text if word not in stopword]
    return text
df1['nonstop']=df1['tokenized'].apply(lambda x: remove_stopwords(x))

df1

#remove_tokens_less_than_three_characters
def filter_text(clean_text):
    filtered_text = []
    nonstop=clean_text
    for token in nonstop:
        if(len(token)>=3):
            filtered_text.append(token)
            
    return filtered_text

df1['filtered'] = df1['nonstop'].apply(lambda x: filter_text(x))

df1

#Appliyng Stemmer
ps = nltk.PorterStemmer()
def stemming(text):
    text = [ps.stem(word) for word in text]
    return text
df1['stemmed'] = df1['filtered'].apply(lambda x: stemming(x))

df1


import nltk
nltk.download('wordnet')

#Appliyng lemmatizer
from nltk.stem import WordNetLemmatizer
def lemmatizing(words):
    lemmatizer =nltk.stem.WordNetLemmatizer()
    return [lemmatizer.lemmatize(word) for word in words]
df1['lemmatized']=df1['filtered'].apply(lambda x: lemmatizing(x))

df1

df2 = df1.drop(['username','created_at','retweets','likes',],axis = 1)

df2

df2.to_csv('cleantokens.csv',encoding="utf-8")



###############BERT model  developement######################
import pandas as pd

#import data
df1=pd.read_csv('sentiment_binary.csv')
df2=pd.read_csv('sentiment_test_binary.csv')

df2.info()

frames=[df1,df2]
df=pd.concat(frames)

df.info()

#install dependencies
pip install transformers

import transformers

from transformers import BertTokenizer, TFBertForSequenceClassification
from transformers import InputExample, InputFeatures

model = TFBertForSequenceClassification.from_pretrained("bert-base-uncased")
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

model.summary()

import tensorflow as tf
import pandas as pd

#split data into train and test data
train=df.sample(frac = 0.8)
test=df.drop(train.index)

train.info()

#input example of the data
InputExample(guid=None,
             text_a = "Hello, world",
             text_b = None,
             label = 1)


#convert our dat to input data for the BERT model
def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN): 
  train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case
                                                          text_a = x[DATA_COLUMN], 
                                                          text_b = None,
                                                          label = x[LABEL_COLUMN]), axis = 1)

  validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case
                                                          text_a = x[DATA_COLUMN], 
                                                          text_b = None,
                                                          label = x[LABEL_COLUMN]), axis = 1)
  
  return train_InputExamples, validation_InputExamples

  train_InputExamples, validation_InputExamples = convert_data_to_examples(train, 
                                                                           test, 
                                                                           'DATA_COLUMN', 
                                                                           'LABEL_COLUMN')
  
def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):
    features = [] # -> will hold InputFeatures to be converted later

    for e in examples:
        # Documentation is really strong for this method, so please take a look at it
        input_dict = tokenizer.encode_plus(
            e.text_a,
            add_special_tokens=True,
            max_length=max_length, # truncates if len(s) > max_length
            return_token_type_ids=True,
            return_attention_mask=True,
            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length
            truncation=True
        )

        input_ids, token_type_ids, attention_mask = (input_dict["input_ids"],
            input_dict["token_type_ids"], input_dict['attention_mask'])

        features.append(
            InputFeatures(
                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label
            )
        )

    def gen():
        for f in features:
            yield (
                {
                    "input_ids": f.input_ids,
                    "attention_mask": f.attention_mask,
                    "token_type_ids": f.token_type_ids,
                },
                f.label,
            )

    return tf.data.Dataset.from_generator(
        gen,
        ({"input_ids": tf.int32, "attention_mask": tf.int32, "token_type_ids": tf.int32}, tf.int64),
        (
            {
                "input_ids": tf.TensorShape([None]),
                "attention_mask": tf.TensorShape([None]),
                "token_type_ids": tf.TensorShape([None]),
            },
            tf.TensorShape([]),
        ),
    )


DATA_COLUMN = 'DATA_COLUMN'
LABEL_COLUMN = 'LABEL_COLUMN'


#test, train and validation data
train_InputExamples, validation_InputExamples = convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN)

train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)
train_data = train_data.shuffle(100).batch(32).repeat(2)

validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)
validation_data = validation_data.batch(32)



#BERT model compilation and fitting

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), 
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), 
              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])

model.fit(train_data, epochs=2, validation_data=validation_data)


#prediction on test data
pred_sentences=test['DATA_COLUMN'].values.tolist()
pred_sentences
tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')
tf_outputs = model(tf_batch)
tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)
labels = ['Negative','Positive']
label = tf.argmax(tf_predictions, axis=1)
label = label.numpy()
for i in range(len(pred_sentences)):
  print(pred_sentences[i], ": \n", labels[label[i]])



 
